[
  {
    "config": {
      "lora_rank": 8,
      "lora_alpha": 16,
      "batch_size": 1,
      "gradient_accumulation": 8,
      "learning_rate": 0.0002,
      "max_seq_length": 1024
    },
    "final_loss": 0.829423709710439,
    "training_time": 151.9802906513214,
    "peak_memory": 5.5869550704956055,
    "trial_name": "trial_01"
  }
]